# **Automatic Translation of Sign Language into Text**

## **Overview**
This project aims to develop an **AI-powered system** that recognizes **sign language gestures** in real-time and converts them into **text**. The model leverages **deep learning**, **computer vision**, and **natural language processing (NLP)** to accurately interpret hand gestures, improving communication accessibility for **deaf and hard-of-hearing individuals**.

---

## **Features**
- **Real-time sign language recognition** using a deep learning-based CNN model.
- **Gesture classification** trained on a dataset of hand gestures.
- **REST API** using **Flask** to accept images and return text predictions.
- **Integration with MediaPipe** for hand landmark detection.
- **MobileNet-based CNN** for fast and efficient gesture classification.
- **JSON-based class mapping** for flexible model expansion.
- **Cross-Origin Resource Sharing (CORS)** enabled for frontend compatibility.

## **Demo**
[![Watch the video](https://raw.githubusercontent.com/mihaivasile17/Automatic-translation-of-sign-language-into-text/main/D:/ProiecteBonus/Automatic-translation-of-sign-language-into-text/git_demo/Thumbnail_hand_recognition.jpeg)](https://raw.githubusercontent.com/mihaivasile17/Automatic-translation-of-sign-language-into-text/main/D:/ProiecteBonus/Automatic-translation-of-sign-language-into-text/git_demo/Video_demo_hand_recognition.mp4)


